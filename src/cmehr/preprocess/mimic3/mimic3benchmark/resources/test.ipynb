{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "subjects_root_path = '/home/fwu/Documents/inferProjects/mimic3-benchmarks/data/root/'\n",
    "folders = os.listdir(subjects_root_path)\n",
    "folders = list((filter(str.isdigit, folders)))\n",
    "\n",
    "all_samples = folders\n",
    "all_samples.sort()\n",
    "\n",
    "np.random.seed(42)\n",
    "train_size = 0.7\n",
    "val_size = 0.1\n",
    "test_size = 0.2\n",
    "\n",
    "train_samples, test_val_samples = train_test_split(all_samples, test_size=1 - train_size, random_state=42)\n",
    "val_samples, test_samples = train_test_split(test_val_samples, test_size=test_size / (test_size + val_size), random_state=42)\n",
    "\n",
    "with open('testset.csv', 'w') as f:\n",
    "    for item in all_samples:\n",
    "        if item in test_samples:\n",
    "            f.write(\"%s,1\\n\" % item)\n",
    "        else:\n",
    "            f.write(\"%s,0\\n\" % item)\n",
    "train_val_samples = list(set(all_samples) - set(test_samples))\n",
    "train_val_samples.sort()\n",
    "\n",
    "# with open('valset.csv', 'w') as f:\n",
    "#     for item in train_val_samples:\n",
    "#         if item in val_samples:\n",
    "        #     f.write(\"%s,1\\n\" % item)\n",
    "        # else:\n",
    "        #     f.write(\"%s,0\\n\" % item)\n",
    "print(\"length of train set: \", len(train_samples))\n",
    "print(\"length of val set: \", len(val_samples))\n",
    "print(\"length of test set: \", len(test_samples))\n",
    "print(\"length of all samples: \", len(all_samples))\n",
    "print(\"length of train_val set: \", len(train_val_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataframe_from_csv(path, header=0, index_col=0):\n",
    "    return pd.read_csv(path, header=header, index_col=index_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"./itemid_to_variable_map.csv\"\n",
    "pr_fn = \"./clinical_predictor_map.json\"\n",
    "variable_column='LEVEL2'\n",
    "\n",
    "def _read_itemid_to_variable_map(fn, pr_fn, variable_column='LEVEL2'):\n",
    "    var_map = dataframe_from_csv(fn, index_col=None).fillna('').astype(str)\n",
    "    with open(pr_fn) as f:\n",
    "        cli_pres_map = json.load(f)\n",
    "    var_map[\"PREDICTOR\"] = \"\"\n",
    "    for category in cli_pres_map.values():\n",
    "        for predictor_name, predictor in category.items():\n",
    "            if predictor[\"LEVEL2\"] != \"NOT-FOUND\":\n",
    "                var_map.loc[var_map[\"LEVEL2\"] == predictor[\"LEVEL2\"], \"PREDICTOR\"] = predictor_name\n",
    "                # print(f\"Index of predictor {predictor_name}: {var_map[var_map.PREDICTOR == predictor_name].index}\")\n",
    "                continue\n",
    "            if predictor[\"LEVEL1\"] != \"NOT-FOUND\":\n",
    "                var_map.loc[var_map[\"LEVEL1\"] == predictor[\"LEVEL1\"], \"PREDICTOR\"] = predictor_name\n",
    "                # print(f\"Index of predictor {predictor_name}: {var_map[var_map.PREDICTOR == predictor_name].index}\")\n",
    "                continue\n",
    "            var_map.loc[var_map[\"MIMIC LABEL\"].isin(predictor[\"MIMIC LABEL\"]), \"PREDICTOR\"] = predictor_name\n",
    "            # print(f\"Index of predictor {predictor_name}: {var_map[var_map.PREDICTOR == predictor_name].index}\")\n",
    "    print(\"length of var_map.PREDICTOR: \", len(var_map.PREDICTOR.unique()))\n",
    "    var_map = var_map[var_map[\"PREDICTOR\"] != \"\"]\n",
    "    var_map['COUNT'] = var_map['COUNT'].apply(lambda x: round(float(x)) if x else 0)\n",
    "    var_map_ex = var_map[(var_map.PREDICTOR == 'Arterial Base Excess') | (var_map.PREDICTOR == 'Phosphorus')]\n",
    "    var_map = var_map[(var_map.STATUS == 'ready') & (var_map.COUNT > 0)] \n",
    "    var_map = pd.concat([var_map, var_map_ex], ignore_index=True)\n",
    "\n",
    "    print(\"length of var_map.PREDICTOR: \", len(var_map.PREDICTOR.unique()))\n",
    "    var_map.ITEMID = var_map.ITEMID.astype(int)      \n",
    "    var_map = var_map[[variable_column, 'ITEMID', 'MIMIC LABEL', 'PREDICTOR']].set_index('ITEMID')\n",
    "    print(\"length of var_map.PREDICTOR: \", len(var_map.PREDICTOR.unique()))\n",
    "    print(var_map.PREDICTOR.unique())\n",
    "\n",
    "    return var_map.rename({variable_column: 'VARIABLE', 'MIMIC LABEL': 'MIMIC_LABEL'}, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# var_map.to_csv(\"itemid_to_variable_map_with_predictor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'LAST_CAREUNIT', 'DBSOURCE',\n",
      "       'INTIME', 'OUTTIME', 'LOS', 'ADMITTIME', 'DISCHTIME', 'DEATHTIME',\n",
      "       'ETHNICITY', 'DIAGNOSIS', 'GENDER', 'DOB', 'DOD', 'AGE',\n",
      "       'MORTALITY_INUNIT', 'MORTALITY', 'MORTALITY_INHOSPITAL',\n",
      "       'READMISSION_30D'],\n",
      "      dtype='object')\n",
      "index:0, Readmission_30D: 0\n",
      "index:1, Readmission_30D: 0\n"
     ]
    }
   ],
   "source": [
    "subject_path = \"../data/root/17\"\n",
    "\n",
    "stays = dataframe_from_csv(os.path.join(subject_path, 'stays.csv'), index_col=None)\n",
    "stays.INTIME = pd.to_datetime(stays.INTIME)\n",
    "stays.OUTTIME = pd.to_datetime(stays.OUTTIME)\n",
    "stays.DOB = pd.to_datetime(stays.DOB)\n",
    "stays.DOD = pd.to_datetime(stays.DOD)\n",
    "stays.DEATHTIME = pd.to_datetime(stays.DEATHTIME)\n",
    "stays.sort_values(by=['INTIME', 'OUTTIME'], inplace=True)\n",
    "stays['READMISSION_30D'] = 0\n",
    "for idx, row in stays.iterrows():\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    prev_row = stays.iloc[idx-1]\n",
    "    prev_outtime = pd.to_datetime(prev_row.OUTTIME)\n",
    "    cur_intime = pd.to_datetime(row.INTIME)\n",
    "    if (cur_intime - prev_outtime).days < 30:\n",
    "        stays.loc[idx-1, 'READMISSION_30D'] = 1\n",
    "print(stays.columns)\n",
    "for idx, row in stays.iterrows():\n",
    "    print(f\"index:{idx}, Readmission_30D: {row.READMISSION_30D}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../../\")\n",
    "from dataset.mimic3.mimic3benchmark.subject import read_stays, read_diagnoses, read_events\n",
    "from dataset.mimic3.mimic3benchmark.preprocessing import map_itemids_to_variables, clean_events , read_itemid_to_variable_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_path:  ../../data/root/test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating over subjects:  38%|███▊      | 2541/6760 [02:04<03:44, 18.75it/s]/home/fwu/Documents/myProjects/cmehr/dataset/mimic3/mimic3benchmark/resources/../../../../dataset/mimic3/mimic3benchmark/util.py:5: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(path, header=header, index_col=index_col)\n",
      "Iterating over subjects: 100%|██████████| 6760/6760 [05:31<00:00, 20.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "subjects_root_path = \"../../data/root/\"\n",
    "fn = \"./itemid_to_variable_map.csv\"\n",
    "pr_fn = \"./clinical_predictor_map.json\"\n",
    "\n",
    "modes = [\"test\"]\n",
    "\n",
    "str_value4predictor = {}\n",
    "ct_map = {}\n",
    "valueom = {}\n",
    "\n",
    "var_map = read_itemid_to_variable_map(fn, pr_fn)\n",
    "# variables = var_map.VARIABLE.unique()\n",
    "variables = var_map.PREDICTOR.unique()\n",
    "\n",
    "for mode in modes:\n",
    "    root_path = os.path.join(subjects_root_path, mode)\n",
    "    print(\"root_path: \", root_path)\n",
    "    for subject_dir in tqdm(os.listdir(root_path), desc='Iterating over subjects'):\n",
    "        dn = os.path.join(root_path, subject_dir)\n",
    "        try:\n",
    "            subject_id = int(subject_dir)\n",
    "            if not os.path.isdir(dn):\n",
    "                raise Exception\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # reading tables of this subject\n",
    "            stays = read_stays(os.path.join(root_path, subject_dir))\n",
    "            diagnoses = read_diagnoses(os.path.join(root_path, subject_dir))\n",
    "            events = read_events(os.path.join(root_path, subject_dir))\n",
    "        except:\n",
    "            sys.stderr.write('Error reading from disk for subject: {}\\n'.format(subject_id))\n",
    "            continue\n",
    "\n",
    "        events = map_itemids_to_variables(events, var_map)\n",
    "        events = clean_events(events)\n",
    "        if events.shape[0] == 0:\n",
    "            # no valid events for this subject\n",
    "            continue\n",
    "#         for idx, row in events.iterrows():\n",
    "#             if row['PREDICTOR'] in variables:\n",
    "#                 if row['PREDICTOR'] not in valueom:\n",
    "#                     valueom[row['PREDICTOR']] = []\n",
    "#                 try :\n",
    "#                     float(row['VALUE'])\n",
    "#                     if row['VALUEUOM'] not in valueom[row['PREDICTOR']]:\n",
    "#                         valueom[row['PREDICTOR']].append(row['VALUEUOM'])\n",
    "#                 except ValueError as e:\n",
    "#                     continue\n",
    "# print(valueom)\n",
    "\n",
    "        variable_column='PREDICTOR'\n",
    "        metadata = events[['CHARTTIME', 'ICUSTAY_ID']].sort_values(by=['CHARTTIME', 'ICUSTAY_ID'])\\\n",
    "                .drop_duplicates(keep='first').set_index('CHARTTIME')\n",
    "        timeseries = events[['CHARTTIME', variable_column, 'VALUE']]\\\n",
    "                        .sort_values(by=['CHARTTIME', variable_column, 'VALUE'], axis=0)\\\n",
    "                        .drop_duplicates(subset=['CHARTTIME', variable_column], keep='last')\n",
    "        timeseries = timeseries.pivot(index='CHARTTIME', columns=variable_column, values='VALUE')\\\n",
    "                        .merge(metadata, left_index=True, right_index=True)\\\n",
    "                        .sort_index(axis=0).reset_index()\n",
    "        for idx, row in timeseries.iterrows():\n",
    "            for v in variables:\n",
    "                if v in row:\n",
    "                    try:\n",
    "                        value = float(row[v])\n",
    "                        # if v in ct_map:\n",
    "                        #     if 'num' not in ct_map[v]:\n",
    "                        #         ct_map[v]['num'] = 1\n",
    "                        #     else:\n",
    "                        #         ct_map[v]['num'] += 1\n",
    "                        # else:\n",
    "                        #     ct_map[v] = {}\n",
    "                        #     ct_map[v]['num'] = 1\n",
    "                    except ValueError as e:\n",
    "                        value = row[v]\n",
    "                        # if v in ct_map:\n",
    "                        #     if 'str' not in ct_map[v]:\n",
    "                        #         ct_map[v]['str'] = 1\n",
    "                        #     else:\n",
    "                        #         ct_map[v]['str'] += 1\n",
    "                        # else:\n",
    "                        #     ct_map[v] = {}\n",
    "                        #     ct_map[v]['str'] = 1\n",
    "                        if v not in str_value4predictor:\n",
    "                            str_value4predictor[v] = []\n",
    "                            str_value4predictor[v].append(value)\n",
    "                        else:\n",
    "                            if value not in str_value4predictor[v]:\n",
    "                                str_value4predictor[v].append(value)\n",
    "print(str_value4predictor)\n",
    "# for k, v in ct_map.items():\n",
    "#     if 'num' in ct_map[k] and 'str' in ct_map[k]:\n",
    "#         ct_map[k]['num_rate'] = ct_map[k]['num'] / (ct_map[k]['num'] + ct_map[k]['str'])\n",
    "# print(ct_map)\n",
    "# json_string = json.dumps(str_value4predictor)\n",
    "# json_string.tojson(\"str_value4predictor.json\", lines=True, indent = 4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_map.PREDICTOR.unique():  ['Anion Gap' 'Serum Bicarbonate' 'Blood Urea Nitrogen' 'Ionized Calcium'\n",
      " 'Serum Chloride' 'Creatinine' 'Diastolic Blood Pressure'\n",
      " 'Fingerstick Glucose' 'Serum Glucose' 'Heart Rate' 'Serum Hematocrit'\n",
      " 'Hemoglobin' 'Magnesium' 'Mean Blood Pressure' 'Oxygen Saturation'\n",
      " 'Arterial Carbon Dioxide Pressure' 'Arterial Oxygen Pressure'\n",
      " 'Arterial pH' 'Platelet Count' 'Serum Potassium' 'Respiratory Rate'\n",
      " 'Serum Sodium' 'Systolic Blood Pressure' 'Body Temperature'\n",
      " 'White Blood Cell Count' 'Arterial Base Excess' 'Phosphorus']\n",
      "var_map_VARIABLE.unique():  ['Anion gap' 'Bicarbonate' 'Blood urea nitrogen' 'Calcium ionized'\n",
      " 'Chloride' 'Creatinine' 'Diastolic blood pressure' 'Glucose' 'Heart Rate'\n",
      " 'Hematocrit' 'Hemoglobin' 'Magnesium' 'Mean blood pressure'\n",
      " 'Oxygen saturation' 'Partial pressure of carbon dioxide'\n",
      " 'Partial pressure of oxygen' 'pH' 'Platelets' 'Potassium serum'\n",
      " 'Respiratory rate' 'Sodium' 'Systolic blood pressure' 'Temperature'\n",
      " 'White blood cell count' 'Arterial Base Excess' '']\n"
     ]
    }
   ],
   "source": [
    "subjects_root_path = \"../../data/root/\"\n",
    "fn = \"./itemid_to_variable_map.csv\"\n",
    "pr_fn = \"./clinical_predictor_map.json\"\n",
    "\n",
    "modes = [\"test\"]\n",
    "\n",
    "str_value4predictor = {}\n",
    "ct_map = {}\n",
    "valueom = {}\n",
    "\n",
    "var_map = read_itemid_to_variable_map(fn, pr_fn)\n",
    "# variables = var_map.VARIABLE.unique()\n",
    "variables = var_map.PREDICTOR.unique()\n",
    "print(\"var_map.PREDICTOR.unique(): \", var_map.PREDICTOR.unique())\n",
    "print(\"var_map_VARIABLE.unique(): \", var_map.VARIABLE.unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
